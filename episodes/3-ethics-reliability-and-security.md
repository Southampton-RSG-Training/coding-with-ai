---
title: "Ethics, Reliability and Security Considerations"
teaching: 10 # teaching time in minutes
exercises: 2 # exercise time in minutes
---

:::::::::::::::::::::::::::::::::::::: questions 

- What are some risks of biased, inaccurate, or unreliable AI-generated outputs?
- How can the use of AI tools compromise data privacy, security, or confidentiality in research and software development?
- What intellectual property and authorship issues emerge when AI contributes to code or written work?
- What are the long-term consequences of researchers relying on AI without developing core coding skills?
- What best practices can ensure that AI is used responsibly, ethically, and transparently in research workflows?

::::::::::::::::::::::::::::::::::::::::::::::::

::::::::::::::::::::::::::::::::::::: objectives

- Describe common sources of bias, inaccuracy, and unreliability in AI-generated outputs.
- Explain data privacy, confidentiality, and security risks associated with using AI tools in coding and research contexts.
- Summarize intellectual property, authorship, and citation considerations related to AI-generated code and text.
- Analyze the potential long-term consequences of researchers relying on AI tools without developing foundational coding skills.
- Examine ethical challenges introduced by AI-assisted research, including accountability, transparency, and reproducibility.
- Assess the appropriateness of AI tool usage in specific research or coding scenarios.
- Apply best practices to mitigate ethical, security, and skills-related risks when using AI in research.
- Develop personal or team-level guidelines for responsible and ethical AI use in coding and data analysis workflows.

::::::::::::::::::::::::::::::::::::::::::::::::

## Overview 

It can be helpful to critically evaluate the risks and implications of AI so that we can feel confident and safe using AI tools to assist with coding in appropriate ways.  In this episode, we'll briefly consider issues related to:

- Accuracy and Reliability of AI-Generated Code
- Data Privacy, Security Risks, and Confidentiality
- Intellectual Property, Authorship, and Citation of AI-Generated Code and Text
- Ethical Considerations in AI-Assisted Research
- De-skilling and Overdependence on AI in Research Computing
- Mitigating Risks: Best Practices for Responsible AI Use in Research



## Accuracy and Reliability of AI-Generated Code

::::::::::::::::::::::::::::::::::::: callout

## AI as an Enthusiastic but Inexperienced Software Developer

When using AI for coding assistance, you can imagine an AI tool as being like a fresh computer science graduate with so much enthusiasm they will never say ‘I can’t do it’, but no practical experience or broad understanding. 

::::::::::::::::::::::::::::::::::::::::::::::::

For researchers, relying on AI-generated code carries significant risks. Incorrect code can lead to flawed results, which may compromise the validity of your research, damage your professional reputation, and even necessitate a paper retraction.

AI coding assistants can produce both random and systematic errors, threatening the reliability and reproducibility of your work. For instance, a [study evaluating the code quality of AI-assisted generation tools](https://arxiv.org/pdf/2304.10778) found that ChatGPT generated correct code  65.2% of the time and GitHub Copilot generated correct code only 46.3% of the time. However, it's important to note that this study was published in 2023, and given the rapid improvements to generative AI over the past few years, it may not be fair to suggest these figures are representative of outputs generated by the current models used by ChatGPT and GitHub Copilot. 

Nonetheless, this study underscores the potential danger of depending solely on AI tools for critical research tasks without carefully reviewing the outputs.

### Outdated Results

If the AI model's training data does not contain recent developments, AI-generated code can be outdated. This can be particularly problematic in fields such as software development and scientific research, where current knowledge is critical.
An AI might suggest a function from an open-source library that hasn't been well-maintained over the past few years. If the library has unpatched security flaws, these could propagate into the project.

For example, an AI might recommend using Python’s urllib module for web requests, because it appears in its training data. However, recent best practices favor the requests library, which is more secure, easier to use, and better maintained. Relying on the outdated suggestion could lead to fragile code or missed features.

### Unreliable Results

AI coding assistants can sometimes produce outputs that are **plausible but incorrect**, a phenomenon often called “hallucination.” When the model lacks relevant training data or encounters an unfamiliar task, it may **invent code or logic** rather than responding with uncertainty. 

Another source of error comes from the **training data itself**. Many large language models have been trained on vast amounts of publicly available code, some of which contains mistakes. As a result, AI-generated code can **inherit these errors** without any indication that they exist.

GPT generates code **one token at a time**, predicting each piece based on the preceding context. Even small token-level mistakes, like a misspelled function name, a missing argument, or an incorrect operator, can produce code that looks correct but fails when executed. For instance:

- Generating `read_csvs()` instead of `read_csv()` will break a script.  
- Omitting a critical argument like `header=None` may silently misread a dataset.  
- Misplaced punctuation or parentheses can introduce subtle bugs that propagate through downstream calculations.

Unchecked token-level errors can escalate into serious problems, including **memory leaks, crashes, or flawed analyses**, which in turn can compromise research results and reproducibility. This underscores the importance of **careful review, testing, and human understanding** of every piece of AI-generated code before it is integrated into a research workflow.


### Transparency and Explainability

Many AI coding assistants function as “black boxes,” offering code suggestions without explaining the reasoning behind them. Without this insight, researchers may find it difficult to verify or trust the proposed solutions, increasing the risk of undetected errors propagating into experimental results.

This lack of transparency has direct implications for research reproducibility. If the logic behind AI-generated code is unclear, other researchers may be unable to replicate your methods or results, even if the code appears to run correctly. There may be subtle errors or undocumented assumptions embedded in AI-generated solutions, which can lead to inconsistencies across experiments or datasets, leading to different results and undermining confidence in your findings.

::::::::::::::::::::::::::::::::::::: callout

## Vibe Coding

Vibe Coding is a term used to describe AI-assisted coding without a structured plan, proper design, or architectural considerations. Decisions are made on the fly, often based on intuition or immediate needs rather than a thoughtful development strategy.

[Andrej Karpathy](https://en.wikipedia.org/wiki/Andrej_Karpathy), co-founder of OpenAI and one of Time Magazine's 100 Most Influential People in AI in 2024, has said about vibe coding: “There’s a new kind of coding, I call ‘vibe coding’, where you fully give in to the vibes, embrace exponentials, and forget that the code even exists. It’s possible because the LLMs … are getting too good.

“When I get error messages I just copy [and] paste them in with no comment, usually that fixes it … I’m building a project or web app, but it’s not really coding – I just see stuff, say stuff, run stuff, and copy paste stuff, and it mostly works.”

This can be fantastic for developing a quite prototype or trying out an idea. However, coding in this way can also lead to some major problems:

- Without planning the structure of your code at the start, programs are likely to become messy and confusing, and this can introduce mistakes into the code.
- Outputs are likely to appear mostly correct and, while obvious errors are usually caught, the subtle mistakes are easy to miss.
- This approach is likely to lead to problems being discovered only during the build or runtime phase instead of during design, which makes them more time-consuming and costly to fix.  

::::::::::::::::::::::::::::::::::::::::::::::::

## Data Privacy, Security Risks, and Confidentiality

### Data Privacy and Confidentiality

AI tools such as chatGPT process and may retain user inputs.  Therefore, it's really important to be cautious that you don't accidentally share confidential code, sensitive datasets or proprietary research methods. Depending on the settings of your AI tool, the information you enter may be reused to improve the AI model and/or could resurface in future outputs, creating risks around intellectual property leakage, confidentiality breaches, or non-compliance with data protection regulations.

When AI tools run in the cloud, your code or data may be transmitted to external servers. This increases the risk of exposure during transfer or through storage breaches. Even if data is anonymised, code structure and comments can still reveal sensitive details about research methods or system design.

### Insecure AI-Generated Code

AI assistants learn from large volumes of existing code, and this code can contain both good and bad coding practices. As a result, they may suggest insecure or outdated practices.  For example, an AI tool might generate code that fails to properly validate user input, leaving the system open to issues like SQL injection or cross-site scripting. This might be difficult to spot, and because the code looks right on the surface, these problems can be missed.

If you are aware of security considerations relevant to your code, include them in your prompt to help ensure the AI generates more secure code.

A [2023 Stanford University study](https://arxiv.org/pdf/2211.03622) found that programmers who used AI assistants often produced less secure code but at the same time, felt more confident that it was secure - a risky combination!


### Third-Party Dependency Risks in AI-Generated Code

AI tools will often generate code that includes external libraries or frameworks without checking whether they are secure and appropriate for your use case. This could lead to vulnerabilities in your code, licensing issues, or compatibility problems, which can cause problems later and be difficult to untangle or debug. 

AI hallucinations of third-party libraries can also be a security risk.  ChatGPT sometimes hallucinates non-existent coding libraries in its outputs. A [study by the security company Vulcan](https://www.securityweek.com/chatgpt-hallucinations-can-be-exploited-to-distribute-malicious-code-packages/) identified a cyberattack technique where criminals could hijack these fake libraries by publishing a malicious package under the name of the non-existent library and hoping developers would install the infected library based on the AI tool’s recommendation.

This practice has become known as 'slopsquatting', a combination of 'AI Slop' and 'typosquatting' (the practice of registering domain names or software package names that are slightly misspelled versions of popular ones to trick users into visiting them or downloading malicious content).

::::::::::::::::::::::::::::::::::::: callout

## Embed a 'security conscience' into the AI

A Security-Focused Guide for AI Code Assistant Instructions was written by the OpenSSF Best Practices and the AI/ML Working Groups.  The guide suggests ways that you can improve the security of AI-generated code by deliberately embedding security expectations into the prompts.  These might include:

- Secure coding best practices that are relevant for your code (e.g. Input validation and output encoding, error handling and logging, secure defaults and configurations, testing for security)
- Reminders of software supply chain security (i.e. security of suggested third-party libraries and dependencies)
- Address relevant platform and runtime security considerations (e.g. operating system, deployment considerations, mobile app security)
- Language-specific security considerations
- Pointing the AI toward relevant security standards and frameworks

Note: Including security expectations in prompts requires knowledge of relevant software security practices, so is outside the scope of this novice course.  However, it's worth bearing in mind if you're interested in developing research software. 


::::::::::::::::::::::::::::::::::::::::::::::::

## Intellectual Property, Authorship, and Citation of AI-Generated Code

### Intellectual Property and Ownership

Intellectual property rights (legal rights that protect creations of the mind) for AI-generated code are currently evolving. 

Currently in the UK, if AI is used to assist a human creator, the content is the human's own intellectual creation and the copyright belongs to the human creator.

If there is no human author, the "author" for copyright purposes is the person "by whom the arrangements necessary for the creation of the work are undertaken." For AI-generated code, this means that if the code is truly without human creative input (for example produced entirely by an AI tool in response to a prompt without the author shaping or selecting the output creatively), the law would treat the person who set up the process (for example the user or developer who provided the prompts and triggered the AI) as the “author” of that computer-generated work.

However, there's ongoing debate about how this practically applies to many forms of AI outputs, including software code, because:

- The statutory language was drafted long before modern AI and may not map cleanly to current AI models.
- UK authorities are actively consulting on whether to adjust or clarify how AI-generated works should be treated in copyright law.
- Ownership can depend on contractual terms (such as developer agreements, employment contracts, or AI tool terms of service), which may assign rights to an employer or platform rather than the individual user.

### Implications for Open Source and Reuse

Separately from ownership of new AI outputs, you may accidentally infringe the copyright of others by generating code that replicates or contains substantial parts of copyrighted works without permission, such as code copied from existing proprietary systems.  AI models are trained on a vast amount of data that may include copyrighted material. While outputs from AI tools are typically novel, there is a risk that generated code or text may closely resemble existing sources. This could lead to copyright infringement claims, particularly in commercial or open-source contexts. If you use AI assistance with coding, you'll need to be cautious about the potential for this to occur. 

If AI-generated code is added to an open-source project, a licensing conflict may be accidentally introduced if the patterns or structures in the code are derived from software with incompatible licences.  Check the project's licence and contribution guidelines before making any AI-assisted contributions.

::::::::::::::::::::::::::::::::::::: callout

## No AI-generated Code Policy for Open-Source Project Cloud Hypervisor

Cloud Hypervisor is an open-source software project that helps large computing systems run multiple programs safely and efficiently at the same time, which is a common requirement in cloud services (services provided over the internet rather than from a local computer). It is maintained by a community of organisations and developers and is made freely available under an open licence. In 2025, the project’s maintainers implemented a no AI-generated code policy for contributions, out of concern that such code might unintentionally include material derived from other software with incompatible licences, creating legal risks for the project and its users. 

In a [post on GitHub](https://github.com/cloud-hypervisor/cloud-hypervisor/blob/v48.0/CONTRIBUTING.md), Cloud Hypervisor's maintainers said: 'Our policy is to decline any contributions known to contain contents generated or derived from using Large Language Models (LLMs). This includes ChatGPT, Gemini, Claude, Copilot and similar tools.'

::::::::::::::::::::::::::::::::::::::::::::::::

### Authorship and Academic Credit

AI tools can influence research outputs, so to what extent should their contribution be acknowledged? AI systems can't be authors but not disclosing their use can misrepresent the nature of the researchers' work and raise academic integrity questions.  

Most publishers agree that AI tools do not qualify for authorship and that human authors are fully accountable for the content they produce.  Publishers are also broadly in agreement that AI use should be disclosed.  


::::::::::::::::::::::::::::::::::::: callout

## Publisher's Stance on AI Use in Academic Research

Summarised from - Rana, N. K. (2025). Generative AI and academic research: A review of the policies from selected HEIs.

- **Cambridge University Press (CUP):** AI tools cannot be credited as authors, and authors remain fully responsible for the accuracy, integrity, and originality of their work.

- **Nature Portfolio (Springer Nature):** AI tools are not permitted as authors, their use must be transparently disclosed, and AI-generated images are generally prohibited due to unresolved legal and ethical concerns. Nature requires disclosure of LLM use in the Methods section (or an equivalent section), rather than in acknowledgements or citations.

- **Elsevier:** Elsevier allows AI-assisted tools for writing support, limited to improving clarity and readability. However, core scholarly activities, such as generating scientific insights, drawing conclusions, or making recommendations, must remain human-led. Elsevier requires authors to declare any AI tool usage and does not allow AI tools to be listed as authors. 

::::::::::::::::::::::::::::::::::::::::::::::::


### Citation

The outputs of AI aren't stable, they're likely to vary depending on prompt wording and the AI model version among other factors, so that can't be reliably cited in the same way as you would cite a research paper or software package. 

Several universities and libraries recommend citing the tool used including the version and date, describing how it was used and clearly distinguishing between AI and human contributions.  Many AI tools now allow chats to be shared through URLs, meaning that specific chats can be cited if that would be appropriate and helpful to readers of the research.

[Duke University Libraries](https://guides.library.duke.edu/citing/AI?utm_source=chatgpt.com) gives the following guidance on how to cite an AI Chat and AI Tool in several different referencing styles.  For example, the APA style would be:

**AI Chat**
 
AI Company Name. (Year, Month Day). Title of chat [Description, such as Generative AI chat]. Tool Name/Model. URL of the chat.

Example: OpenAI. (2025, August 21). *High school grammar concepts* [Generative AI chat]. ChatGPT. https://chatgpt.com/share/68a77b60-0ee4-800c-9acc-cd3fd573c311

**AI Tool**
 
AI Company Name. (Year). Tool Name/Model [Description: e.g., Large language model]. URL of the tool

Example: OpenAI. (2025). *ChatGPT* [Large language model]. https://chatgpt.com/



## Ethical Considerations in AI-Assisted Research

### Bias in AI Systems

AI model outputs reflect any biases present in their training data. This may result in outputs that:

- Favour certain demographic groups over others
- Embedding biased assumptions into code or documentation

An Ethical AI Framework was proposed by [Vilas Dhar](https://en.wikipedia.org/wiki/Vilas_Dhar), a social entrepreneur focused on the ethical use of AI and president of the Patrick J. McGovern Foundation.  The framework is built on three pillars:

1. Responsible data practices
2. Boundaries around safe and appropriate use
3. Understand how tools generate their recommendations. When AI systems operate as black boxes, it becomes difficult to assess validity, fairness, or risk.

In a research setting applying this framework might look like:

1. Checking your data: AI reflects the datasets it learned from. For example, an AI trained on outdated species measurements might misclassify new observations.
2. Setting boundaries on key decisions: Decide which tasks are appropriate for AI and which should always be done by humans.
3. Understanding the AI’s reasoning: Ask how the AI arrived at its suggestion. If a statistical model or code snippet doesn’t make sense, investigate before using it.

### Avoid Over-Trust in AI and Keep Humans in Control

One of the most common, and least discussed, biases in AI use is the tendency to over-value AI-generated outputs. Outputs from GPT systems often have an authoritative tone, influencing the extent to which you believe them, but it's important to remember that they do not possess understanding, intent, or ethical awareness.  Human judgement must remain at the centre of research, with AI tools used to support humans rather than override them.

Researchers should avoid placing AI on a pedestal and instead:

- Treat AI outputs as suggestions, not truths
- Value the role of human creativity and critical thinking
- Remember that the researcher should take responsibility for an AI-generated code or other content they use


## De-skilling and Overdependence on AI in Research Computing

AI tools can significantly enhance productivity in research computing, but excessive reliance on them introduces risks to research quality, integrity, and long-term capability.

### Risks of De-skilling

Over-reliance on AI for coding can lead researchers to lose, or fail to develop, core technical skills. When you do not fully understand the code you use, you can't reliably verify whether AI-generated outputs are correct, robust, or appropriate for their research context. As a result, this may reduce confidence in the validity of published findings. 

In addition to technical skills, excessive use of AI may also reduce your critical evaluation skills. AI-generated code often appears plausible, confident and well-structured, which can encourage blind acceptance. This means that subtle errors, hidden assumptions, or inappropriate methods may go unnoticed, particularly when your code gets more complex.

There are also long-term implications for the research community. If researchers become dependent on AI tools for fundamental software development tasks, institutions risk losing the collective ability to design, build, and maintain research software independently. This creates problem if tools become unavailable, restricted, or unsuitable for specific research needs.

### AI as a Tool, Not a Replacement

It can be helpful to think of AI  as a tool that can help with and accelerate certain tasks. Used thoughtfully, AI can help explore ideas, generate boilerplate code, or suggest alternative approaches.

However, maintaining human creativity and judgement is especially critical in research, where novelty, insight, and deep understanding often matter more than speed. AI can assist, but it cannot define research questions, interpret results, or make ethical and methodological decisions.

## Mitigating Risks: Best Practices for Responsible AI Use in Research

Responsible use of AI for coding assistance requires a combination of ethical awareness, technical safeguards, and disciplined research practice. Here are some examples of ethical best practices, practices to support research reproducibility and scientific validity, and some security measures that you may put in place when using AI to assist with research coding.

### Ethical Best Practices

- **Maintain human oversight:** Researchers must critically evaluate all AI-generated outputs, remaining alert to potential bias, errors, or inappropriate assumptions.
- **Test and validate rigorously:** AI-generated code should be treated as untrusted by default. Apply thorough testing and validation to ensure correctness, reliability, and fitness for purpose.
- **Protect sensitive data:** Avoid submitting proprietary code, confidential data, or sensitive research materials to online AI tools. If you use these materials for your research and have decided to use AI, you may want to investigate locally hosted or offline AI assistants to reduce data exposure risks.
- **Define clear usage guidelines:** Establish and follow explicit policies for AI use in research computing. These may draw on recognised frameworks such as the [ACM Code of Ethics](https://www.acm.org/code-of-ethics) or the [European Commission’s Ethical Guidelines on AI](https://digital-strategy.ec.europa.eu/en/library/ethics-guidelines-trustworthy-ai), alongside institution-specific policies.

### Practices Supporting Reproducibility and Scientific Validity

To maintain transparency, reproducibility, and scientific validity when using AI tools, researchers should:

- **Document AI involvement:** Record when, how, and why AI-generated suggestions were used or modified.
- **Validate against known results:** Test AI-generated code using benchmarks, reference datasets, or established methods before integration.
- **Combine AI with domain expertise:** Use AI to support human judgement and subject-matter knowledge, rather than replacing them.

### Security Measures

- **Code review:** Review all AI-generated code, ideally using standard code review processes, to identify vulnerabilities, logic errors, or unsafe practices.
- **Secure development practices:** If you're working on a larger piece of research software, integrate security testing tools into development workflows and ensure researchers are trained in secure coding principles.
- **Protect access and data:** Use access controls and encryption to safeguard codebases and datasets from unauthorised access by AI tools.


::::::::::::::::::::::::::::::::::::: challenge 

## Personal Ethics and Security Policy

Write a short personal policy outlining how you will use AI tools responsibly to assist with coding. Include at least **three clear guidelines**.

:::::::::::::::::::::::: solution 
Your guidelines could include:

- Always document when and how AI tools are used.
- Make sure I understand any code generated by AI before using it for my research.
- Never input sensitive, personal, or proprietary data into AI systems.
- Take full responsibility for my research code, even when it is AI generated. 
- Maintain my critical thinking and decision making skills, never allow AI to do these things for me.

:::::::::::::::::::::::::::::::::

::::::::::::::::::::::::::::::::::::::::::::::::

::::::::::::::::::::::::::::::::::::: keypoints 
- AI-generated code is not fully reliable: it may contain subtle errors, outdated functions, or fabricated solutions (hallucinations) that compromise research validity and reproducibility.  
- Vibe coding (AI-assisted coding without planning) can produce messy, error-prone programs. Structured development and verification remain essential. 
- Using AI tools can create data privacy, confidentiality, and security risks, especially when submitting sensitive datasets or proprietary code to cloud-based AI services.  
- AI may suggest insecure or outdated coding practices. To mitigate the risk you could embed security expectations in prompts and review outputs critically.  
- Be aware of the evolving issues surrounding intellectual property, authorship, and citation of AI-generated code.   
- Over-reliance on AI can lead to de-skilling, reducing researchers’ coding proficiency, critical thinking, and long-term ability to maintain software.  
- Ethical AI use requires human oversight, responsible data practices, defined boundaries, transparency, and validation of AI-generated outputs.  
- It could be helpful for researchers to develop personal or team-level AI ethics and security policies.

::::::::::::::::::::::::::::::::::::::::::::::::


## References

- [AI-Assisted Coding with Codium, Ethical and Security Considerations](https://carpentries-incubator.github.io/gen-ai-coding/3-ethical-and-security-considerations.html)
- [Perry, N., Srivastava, M., Kumar, D., & Boneh, D. (2023, November). Do users write more insecure code with ai assistants?. In Proceedings of the 2023 ACM SIGSAC conference on computer and communications security (pp. 2785-2799)](https://arxiv.org/pdf/2211.03622)
- [Yetiştiren, B., Özsoy, I., Ayerdem, M., & Tüzün, E. (2023). Evaluating the code quality of ai-assisted code generation tools: An empirical study on github copilot, amazon codewhisperer, and chatgpt. arXiv preprint arXiv:2304.10778.](https://arxiv.org/pdf/2304.10778)
- [Now you don’t even need code to be a programmer. But you do still need expertise](https://www.theguardian.com/technology/2025/mar/16/ai-software-coding-programmer-expertise-jobs-threat)
- [UK Government Consultation on Copyright and Artificial Intelligence](https://www.gov.uk/government/consultations/copyright-and-artificial-intelligence/copyright-and-artificial-intelligence?utm_source=chatgpt.com)
- [Rana, N. K. (2025). Generative AI and academic research: A review of the policies from selected HEIs. Higher Education for the Future, 12(1), 97-113.](https://journals.sagepub.com/doi/full/10.1177/23476311241303800?casa_token=-dV5W8PZZxgAAAAA%3AqIIeQvYk9BeLbYg7K1kNa04fue-7bu62xdGeeaCMovWdJxa3YZ7NsruZkuwdrboBpAjktY1AlIU)
- [Duke University Libraries guide to citing artificial intelligence](https://guides.library.duke.edu/citing/AI?utm_source=chatgpt.com)
- [Cloud Hypervisor says no to AI code - but it probably won't help in this day and age](https://www.techradar.com/pro/cloud-hypervisor-says-no-to-ai-code-but-it-probably-wont-help-in-this-day-and-age?utm_source=chatgpt.com)
- [Ethical AI Framework by Vilas Dhar](https://cgsandesh.medium.com/ethical-ai-framework-by-vilas-dhar-6c3b243d587c)
- [What is Generative AI? - LinkedIn Learning](https://www.linkedin.com/learning/what-is-generative-ai)
- [Security Focused Guide for AI Code Assistant Instructions](https://best.openssf.org/Security-Focused-Guide-for-AI-Code-Assistant-Instructions)
- [ChatGPT Hallucinations Can Be Exploited to Distribute Malicious Code Packages](https://www.securityweek.com/chatgpt-hallucinations-can-be-exploited-to-distribute-malicious-code-packages/)
